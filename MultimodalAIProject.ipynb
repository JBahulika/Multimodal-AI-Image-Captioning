{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dde2ef-fb54-4965-9ded-54b43f121f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#Caption Loading Utility\n",
    "# ==============================================================================\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"Normalize for dedupe: collapse spaces + lowercase.\"\"\"\n",
    "    return \" \".join(s.split()).strip().casefold()\n",
    "\n",
    "def load_captions(source: str | None = None, folder: str | None = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Load captions from a single JSON file (source) or from all JSON files in a folder.\n",
    "    Dedupe while preserving original text & order of first occurrence.\n",
    "    \"\"\"\n",
    "    if not source and not folder:\n",
    "        raise ValueError(\"Provide either `source` (file) or `folder` (directory).\")\n",
    "\n",
    "    captions: list[str] = []\n",
    "\n",
    "    if source:\n",
    "        p = Path(source)\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Captions file not found: {p.resolve()}\")\n",
    "        with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            captions.extend(json.load(f))\n",
    "\n",
    "    if folder:\n",
    "        p = Path(folder)\n",
    "        if not p.is_dir():\n",
    "            raise FileNotFoundError(f\"Folder not found: {p.resolve()}\")\n",
    "        for jf in sorted(p.glob(\"*.json\")):\n",
    "            with jf.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                captions.extend(json.load(f))\n",
    "\n",
    "    # Dedupe by normalized key\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for c in captions:\n",
    "        if not isinstance(c, str):\n",
    "            continue\n",
    "        key = _norm(c)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(c.strip())\n",
    "\n",
    "    if not unique:\n",
    "        raise ValueError(\"No captions loaded (file(s) empty?).\")\n",
    "\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74bcb2-6408-42ef-81d1-402e201ce749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Define Path to Your Captions File\n",
    "# ==============================================================================\n",
    "# Point this to your JSON file containing the list of candidate captions.\n",
    "CAPTIONS_PATH = \"captions_set_01.json\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30c345-9ee0-489f-ac03-eea26e4f3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#Core Functions for Embedding and Randomized Diverse Selection\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def pick_caption_weighted_topk(image_features_np, caption_features_np, captions, top_k=5, temperature=1.0):\n",
    "   \n",
    "    #Picks a caption from the top_k most similar captions using weighted random sampling.\n",
    "    \n",
    "    sims = cosine_similarity(image_features_np, caption_features_np)[0]\n",
    "    top_indices = np.argsort(sims)[-top_k:][::-1]\n",
    "    top_scores = sims[top_indices]\n",
    "    if temperature <= 0:\n",
    "        chosen_idx = top_indices[0]\n",
    "    else:\n",
    "        scaled_scores = top_scores / temperature\n",
    "        scaled_scores -= np.max(scaled_scores)\n",
    "        exp_scores = np.exp(scaled_scores)\n",
    "        probs = exp_scores / np.sum(exp_scores)\n",
    "        chosen_idx = np.random.choice(top_indices, p=probs)\n",
    "    return captions[chosen_idx], top_indices.tolist(), top_scores.tolist()\n",
    "\n",
    "def pick_diverse_caption(image_features_np, caption_features_np, captions, top_k=5, temperature=1.5, pool_size=30, diversity_threshold=0.97):\n",
    "    \"\"\"\n",
    "    Selects a randomized, diverse set of captions before making a weighted random choice.\n",
    "    \"\"\"\n",
    "    sims = cosine_similarity(image_features_np, caption_features_np)[0]\n",
    "    \n",
    "    # 1. Get a large pool of initial candidates\n",
    "    initial_indices = np.argsort(sims)[-pool_size:][::-1]\n",
    "    \n",
    "    if len(initial_indices) == 0:\n",
    "        return \"No captions found\", [], []\n",
    "\n",
    "    # 2. Start the diverse list with the absolute best caption\n",
    "    diverse_indices = [initial_indices[0]]\n",
    "    \n",
    "    # 3. Shuffle the rest of the candidates to introduce randomness\n",
    "    remaining_indices = list(initial_indices[1:])\n",
    "    random.shuffle(remaining_indices)\n",
    "    \n",
    "    # 4. Iterate through the shuffled list to find other diverse candidates\n",
    "    for idx in remaining_indices:\n",
    "        if len(diverse_indices) >= top_k:\n",
    "            break\n",
    "            \n",
    "        current_embedding = caption_features_np[idx].reshape(1, -1)\n",
    "        selected_embeddings = caption_features_np[diverse_indices]\n",
    "        \n",
    "        # Check similarity against captions already selected\n",
    "        similarity_to_selected = cosine_similarity(current_embedding, selected_embeddings)[0]\n",
    "        \n",
    "        if np.max(similarity_to_selected) < diversity_threshold:\n",
    "            diverse_indices.append(idx)\n",
    "\n",
    "    # 5. Re-sort the final diverse list by similarity for display\n",
    "    final_scores = sims[diverse_indices]\n",
    "    sorted_order = np.argsort(final_scores)[::-1]\n",
    "    final_diverse_indices = np.array(diverse_indices)[sorted_order]\n",
    "    final_diverse_scores = final_scores[sorted_order]\n",
    "    \n",
    "    if len(final_diverse_indices) == 0:\n",
    "         return \"Could not find any diverse captions.\", [], []\n",
    "\n",
    "    # 6. Perform temperature-based weighted choice on the final sorted DIVERSE set\n",
    "    scaled_scores = final_diverse_scores / temperature\n",
    "    scaled_scores -= np.max(scaled_scores)\n",
    "    exp_scores = np.exp(scaled_scores)\n",
    "    probs = exp_scores / np.sum(exp_scores)\n",
    "    \n",
    "    chosen_idx = np.random.choice(final_diverse_indices, p=probs)\n",
    "    \n",
    "    return captions[chosen_idx], final_diverse_indices.tolist(), final_diverse_scores.tolist()\n",
    "\n",
    "def compute_caption_embeddings(captions, clip_model, processor):\n",
    "    \"\"\"Compute CLIP text embeddings for all captions and return a numpy array.\"\"\"\n",
    "    clip_model.eval()\n",
    "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    text_inputs = {k: v.to(clip_model.device) for k, v in text_inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(**text_inputs)\n",
    "    return text_features.detach().cpu().numpy()\n",
    "\n",
    "def compute_image_embedding(image, clip_model, processor):\n",
    "    \"\"\"Compute CLIP image embedding for a PIL image and return a numpy array.\"\"\"\n",
    "    clip_model.eval()\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(clip_model.device)\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].to(clip_model.dtype)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "    return image_features.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f1f5f-2a9c-4f5c-9fcc-ac79f172b8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: laion/CLIP-ViT-H-14-laion2B-s32B-b79K onto CPU\n",
      "Model loaded successfully.\n",
      "Computing caption embeddings...\n",
      "600 caption embeddings computed.\n",
      "\n",
      "Chosen caption (diverse selection):\n",
      "-> Energy speaks.\n",
      "\n",
      "Top-5 DIVERSE candidates (sorted by similarity):\n",
      "1. Living between to‑do and ta‑da. (sim=0.2189)\n",
      "2. Peace looks like this. (sim=0.1930)\n",
      "3. Little by little becomes a lot. (sim=0.1868)\n",
      "4. Keeping it real and really kind. (sim=0.1863)\n",
      "5. Sky is the limit (sim=0.1853)\n",
      "6. Energy speaks. (sim=0.1851)\n",
      "7. Proof of life: this post. (sim=0.1845)\n",
      "8. Creating space for serendipity. (sim=0.1808)\n",
      "9. Optimism looks good on everyone. (sim=0.1806)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#Main Demo - With Diversity Filtering\n",
    "# ==============================================================================\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# 1) Define the model ID\n",
    "MODEL_ID = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "\n",
    "# 2) Set up device and data type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "# 3) Load the model and processor\n",
    "print(f\"Loading model: {MODEL_ID} onto {device.upper()}\")\n",
    "clip_model = CLIPModel.from_pretrained(MODEL_ID, torch_dtype=torch_dtype).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# 4) Load captions\n",
    "candidate_captions = load_captions(source=CAPTIONS_PATH)\n",
    "\n",
    "# 5) Compute caption embeddings\n",
    "print(\"Computing caption embeddings...\")\n",
    "caption_embeds = compute_caption_embeddings(candidate_captions, clip_model, processor)\n",
    "print(f\"{len(caption_embeds)} caption embeddings computed.\")\n",
    "\n",
    "# 6) Load your image\n",
    "image_path = \"tree.jpg\"  # <-- Make sure this is your image file\n",
    "try:\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_embed = compute_image_embedding(image, clip_model, processor)\n",
    "\n",
    "    # 7) Pick a caption using the NEW DIVERSITY-AWARE function\n",
    "    # You can tune temperature and diversity_threshold\n",
    "    chosen, top_idx, top_scores = pick_diverse_caption(\n",
    "        image_embed, \n",
    "        caption_embeds, \n",
    "        candidate_captions, \n",
    "        top_k=9, \n",
    "        temperature=1.5,\n",
    "        diversity_threshold=0.95 # Lower this (e.g., 0.95) to force even more diversity\n",
    "    )\n",
    "\n",
    "    # 8) Display results\n",
    "    print(f\"\\nChosen caption (diverse selection):\\n-> {chosen}\")\n",
    "    \n",
    "    print(\"\\nTop-5 DIVERSE candidates (sorted by similarity):\")\n",
    "    # Note: The indices in top_idx are now from the original list of 600 captions\n",
    "    diverse_captions = [candidate_captions[i] for i in top_idx]\n",
    "    for rank, (caption, score) in enumerate(zip(diverse_captions, top_scores), start=1):\n",
    "        print(f\"{rank}. {caption} (sim={score:.4f})\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nERROR: Image file not found at '{image_path}'. Please check the path and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf8e20-0c41-4d0a-9499-8404089111bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99e592-2068-4bb6-b7c6-fe86402efb28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
